{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNpsGWc2NAs44z+5WWg0Kaq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nebiyu01/NuclearSpecklesQuantification/blob/main/MLProject_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vaderSentiment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJ4kgGME1rsJ",
        "outputId": "15afdcd4-2204-4656-a781-13a126c8d793"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from vaderSentiment) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "tGUJ1hGewVJv"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4t4KPB42Bl4",
        "outputId": "68d117ce-1aa2-4772-f7ae-cd0212cb17cf"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('Tesla.csv')\n",
        "\n",
        "coulmns_to_keep = ['tweet', 'language', 'date', 'nlikes', 'nreplies', 'nretweets']\n",
        "\n",
        "cleaned_data = data[coulmns_to_keep]\n",
        "\n",
        "print(cleaned_data.head())\n",
        "\n",
        "cleaned_data.to_csv('cleaned_Tesla.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7XqmjNpyFTl",
        "outputId": "14b41938-3922-4da9-9bfa-23f2cbefea0b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               tweet language  \\\n",
            "0  @GailAlfarATX @elonmusk @Tesla @teslacn @Tesla...       en   \n",
            "1  @elonmusk @GailAlfarATX @Tesla @teslacn @Tesla...       en   \n",
            "2  @elonmusk #Think about buying a country , #Mex...       en   \n",
            "3  @get_innocuous Actual receipts, and yet you ha...       en   \n",
            "4  Tesla wall battery for the save! Power went ou...       en   \n",
            "\n",
            "                  date  nlikes  nreplies  nretweets  \n",
            "0  2022-07-11 17:06:24       0         0          0  \n",
            "1  2022-07-11 17:06:21       0         0          0  \n",
            "2  2022-07-11 17:06:20       0         0          0  \n",
            "3  2022-07-11 17:06:12       0         0          0  \n",
            "4  2022-07-11 17:06:09       0         0          0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle missing values\n",
        "cleaned_data = cleaned_data.dropna(subset=['tweet']).copy()  # Explicitly create a copy to avoid SettingWithCopyWarning\n",
        "\n",
        "# Tokenization, stopword removal, and text normalization\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "a7iQ1u_Lx6-y"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Remove URLs, mentions, hashtags, and special characters\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)  # Remove URLs\n",
        "    text = re.sub(r\"@\\w+\", \"\", text)  # Remove mentions\n",
        "    text = re.sub(r\"#\\w+\", \"\", text)  # Remove hashtags\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  # Remove special characters and numbers\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords and lemmatize\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Join tokens back into a single string\n",
        "    return \" \".join(tokens)"
      ],
      "metadata": {
        "id": "W4dco3F329-I"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_data['cleaned_tweet'] = cleaned_data['tweet'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "ij8hKDma3CjZ"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize VADER sentiment analyzer\n",
        "sia = SentimentIntensityAnalyzer()"
      ],
      "metadata": {
        "id": "ImmW1KeQ6eAa"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentiment(text):\n",
        "    scores = sia.polarity_scores(text)\n",
        "    if scores['compound'] >= 0.05:\n",
        "        return 'Positive'\n",
        "    elif scores['compound'] <= -0.05:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n"
      ],
      "metadata": {
        "id": "Zu2_c4mu65wK"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_data['sentiment'] = cleaned_data['cleaned_tweet'].apply(get_sentiment)\n",
        "print(cleaned_data[['tweet', 'cleaned_tweet', 'sentiment']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeOGaYVd6-Uq",
        "outputId": "e8663a69-2976-4a51-9932-a3e22da77368"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               tweet  \\\n",
            "0  @GailAlfarATX @elonmusk @Tesla @teslacn @Tesla...   \n",
            "1  @elonmusk @GailAlfarATX @Tesla @teslacn @Tesla...   \n",
            "2  @elonmusk #Think about buying a country , #Mex...   \n",
            "3  @get_innocuous Actual receipts, and yet you ha...   \n",
            "4  Tesla wall battery for the save! Power went ou...   \n",
            "\n",
            "                                       cleaned_tweet sentiment  \n",
            "0  six still live home homeschooled taught selfed...  Negative  \n",
            "1                            go dozen kid missing go  Negative  \n",
            "2  buying country could turn richest country worl...  Positive  \n",
            "3  actual receipt yet havent asked anyone buy tes...  Positive  \n",
            "4    tesla wall battery save power went still run ac  Positive  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_data.to_csv('sentiment_analyzed_Tesla.csv', index=False)\n"
      ],
      "metadata": {
        "id": "bFMEuR837FlH"
      },
      "execution_count": 51,
      "outputs": []
    }
  ]
}